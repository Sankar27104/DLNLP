{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMeJ38CQ/8jwLjQ2ps0L7kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sankar27104/DLNLP/blob/main/DLNLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of preprocessing of Text with NLTK (Tokenization,Stemming, Lemmatization and removal of stop words in NLP."
      ],
      "metadata": {
        "id": "jhFkb2ufVS9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZATION :\n",
        "###Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP preprocessing."
      ],
      "metadata": {
        "id": "rpk-iEQwWIsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khtyp1RkVOqD"
      },
      "outputs": [],
      "source": [
        "text = \"Sankar and Sussan are such a damn coders\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEMMING\n",
        "###Stemming generates the base word from the word by removing the affixes of the word. It must be noted that stemmers might not always result in semantically meaningful base words.\n"
      ],
      "metadata": {
        "id": "6DkpYHzpWYzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "sentence = \"Sankar and Sussan are such a damn coders\"\n",
        "for word in sentence.split():\n",
        " print(ps.stem(word))"
      ],
      "metadata": {
        "id": "XyBrdUneWgGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LEMMATIZATION\n",
        "###Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out to the base form of any word which will be meaningful in nature. The base from here is called the Lemma.\n"
      ],
      "metadata": {
        "id": "en9EA2m8WtZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"read\", pos= \"v\"))\n",
        "print(lemmatizer.lemmatize(\"teaching\", pos= \"v\"))"
      ],
      "metadata": {
        "id": "pVtoT3u9W2ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##REMOVAL OF STOP WORDS\n",
        "### Stop word removal is one of the most commonly used preprocessing steps across different NLP applications. The idea is simply removing the words that occur commonly across all the documents in the corpus."
      ],
      "metadata": {
        "id": "hNMd9TxdXKuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "sF77Mo50XRLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}